# ==============================================================================
# æ­¥éª¤ 0: å¯¼å…¥ä¸è®¾ç½®
# ==============================================================================
import numpy as np
from sentence_transformers import SentenceTransformer
import random
import copy
import hashlib
import json
from typing import Dict, Any, List, Tuple
import math
import torch
import torch.nn as nn
import torch.optim as optim
from collections import namedtuple, deque
import re
import os
import google.generativeai as genai

# --- åŠ è½½æœ¬åœ°ç¼–ç å™¨æ¨¡å‹ ---
print("æ­£åœ¨åŠ è½½ SentenceTransformer æ¨¡å‹...")
ENCODER_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
print("æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚")

# --- é…ç½®Google Gemini API ---
try:
    # ä¼˜å…ˆå°è¯•ä»Google Colabçš„SecretsåŠŸèƒ½ä¸­å®‰å…¨åœ°è·å–APIå¯†é’¥
    from google.colab import userdata
    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
    print("å·²ä» Colab Secrets åŠ è½½ API å¯†é’¥ã€‚")
except ImportError:
    # å¦‚æœä¸åœ¨Colabç¯å¢ƒï¼Œåˆ™å°è¯•ä»ç¯å¢ƒå˜é‡æˆ–ä»£ç ä¸­ç›´æ¥è¯»å–
    # ä¸ºäº†å®‰å…¨ï¼Œå¼ºçƒˆå»ºè®®ä½ ä½¿ç”¨ç¯å¢ƒå˜é‡æ¥è®¾ç½®å¯†é’¥
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "åœ¨è¿™é‡Œå¡«å…¥ä½ çš„APIå¯†é’¥")
    if GOOGLE_API_KEY == "åœ¨è¿™é‡Œå¡«å…¥ä½ çš„APIå¯†é’¥":
        print("è­¦å‘Šï¼šè¯·åœ¨ä»£ç ä¸­æˆ–ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ä½ çš„ GOOGLE_API_KEYã€‚")

# ä½¿ç”¨è·å–åˆ°çš„å¯†é’¥é…ç½®genaiåº“
genai.configure(api_key=GOOGLE_API_KEY)

print("=" * 60)

# ==============================================================================
# æ­¥éª¤ 1: LLM API è®¾ç½®
# ==============================================================================
try:
    from google.colab import userdata
    # ä¼˜å…ˆä»Colab Secretsè·å–APIå¯†é’¥
    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
    print("å·²ä» Colab Secrets åŠ è½½ API å¯†é’¥ã€‚")
except ImportError:
    # å¦‚æœä¸åœ¨Colabç¯å¢ƒï¼Œè¯·æ‰‹åŠ¨å¡«å…¥æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "åœ¨è¿™é‡Œå¡«å…¥ä½ çš„APIå¯†é’¥")
    if GOOGLE_API_KEY == "åœ¨è¿™é‡Œå¡«å…¥ä½ çš„APIå¯†é’¥":
        print("è­¦å‘Šï¼šè¯·åœ¨ä»£ç ä¸­æˆ–ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ä½ çš„ GOOGLE_API_KEYã€‚")

genai.configure(api_key=GOOGLE_API_KEY)


def call_generator_llm(prompt_dict: Dict[str, Any]) -> str:
    prompt_string = f"Please execute the following task described in the JSON prompt:\n\n{json.dumps(prompt_dict, indent=2)}"
    model = genai.GenerativeModel('gemini-1.0-pro')
    try:
        response = model.generate_content(prompt_string)
        return response.text
    except Exception as e:
        print(f"è°ƒç”¨ç”Ÿæˆæ¨¡å‹æ—¶å‡ºé”™: {e}")
        return ""

def call_evaluator_llm(original_task: str, generated_response: str) -> int:
    model = genai.GenerativeModel('gemini-1.5-pro-latest')
    evaluator_prompt = f"""
    As an expert evaluator, your task is to rate the quality of a generated response based on an original task.
    Please provide a score from 1 to 10, where 1 is extremely poor and 10 is excellent.
    Your evaluation should consider clarity, relevance, and adherence to any constraints mentioned in the task.
    Return ONLY the score in the format "Score: X/10".

    ---
    **Original Task:**
    "{original_task}"

    **Generated Response to Evaluate:**
    "{generated_response}"
    ---

    **Your objective evaluation:**
    """
    try:
        response = model.generate_content(evaluator_prompt)
        match = re.search(r'Score:\s*(\d+)/10', response.text)
        if match:
            return int(match.group(1))
        else:
            # å°è¯•ç›´æ¥è§£ææ•°å­—
            match_num = re.search(r'(\d+)/10', response.text)
            if match_num:
                return int(match_num.group(1))
            print(f"æ— æ³•ä»è¯„ä¼°å™¨å“åº”ä¸­è§£æåˆ†æ•°: {response.text}")
            return 0
    except Exception as e:
        print(f"è°ƒç”¨è¯„ä¼°æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return 0

# (æ­¤å¤„çœç•¥ActionFactory, DQN, ReplayMemory, DQNAgentç±»çš„ä»£ç ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰æ”¹å˜)
# (è¯·å°†ä¸‹é¢ä¿®æ”¹åçš„ PromptOptimizerEnv æ›¿æ¢æ‰ä½ åŸæœ‰çš„ç‰ˆæœ¬)
# ==============================================================================
# æ¨¡å— A: é…ç½®ä¸åŠ¨ä½œç©ºé—´ (æœªæ”¹å˜ï¼Œæ­¤å¤„çœç•¥)
# ==============================================================================
class ActionFactory:
    ACTION_LIBRARIES={"roles":["You are a world-renowned historian specializing in the Renaissance.","You are a cutting-edge astrophysicist from MIT.","You are a Michelin 3-star chef with expertise in molecular gastronomy.","You are a cynical but brilliant detective from a noir film."],"constraints":["Explain your reasoning step-by-step.","The final answer must be a single paragraph.","Avoid technical jargon and use simple language.","Output the result in a JSON format with keys 'item' and 'description'.","Do not mention your own identity as an AI model."],"examples":[{"input":"Company: Apple Inc.","output":"Industry: Technology"},{"input":"Company: Toyota","output":"Industry: Automotive"}],"tones":["in a formal and professional tone","in a friendly and conversational tone","in a witty and slightly sarcastic tone","in a way that a complete beginner can understand"]}
    PARAPHRASE_TASK=0
    CHANGE_TONE=1
    ADD_EXPERT_ROLE=2
    ADD_CONSTRAINT=3
    ADD_FEW_SHOT_EXAMPLE=4
    ACTION_MAP={PARAPHRASE_TASK:"paraphrase_task",CHANGE_TONE:"change_tone",ADD_EXPERT_ROLE:"add_expert_role",ADD_CONSTRAINT:"add_constraint",ADD_FEW_SHOT_EXAMPLE:"add_few_shot_example",}
    @staticmethod
    def _get_unique_choice(current_items:List[Any],library:List[Any])->Any:
        potential_choices=[item for item in library if item not in current_items]
        if not potential_choices:return random.choice(library)
        return random.choice(potential_choices)
    @staticmethod
    def apply_action(action_id:int,prompt:Dict[str,Any])->Dict[str,Any]:
        new_prompt=copy.deepcopy(prompt)
        if action_id==ActionFactory.PARAPHRASE_TASK:new_prompt['task']=f"[Paraphrased] {new_prompt.get('task','')}"
        elif action_id==ActionFactory.CHANGE_TONE:
            chosen_tone=random.choice(ActionFactory.ACTION_LIBRARIES["tones"])
            new_prompt['task']=f"{new_prompt.get('task','')}, {chosen_tone}."
        elif action_id==ActionFactory.ADD_EXPERT_ROLE:new_prompt['role']=random.choice(ActionFactory.ACTION_LIBRARIES["roles"])
        elif action_id==ActionFactory.ADD_CONSTRAINT:
            constraints=new_prompt.setdefault('constraints',[])
            choice=ActionFactory._get_unique_choice(constraints,ActionFactory.ACTION_LIBRARIES["constraints"])
            constraints.append(choice)
        elif action_id==ActionFactory.ADD_FEW_SHOT_EXAMPLE:
            examples=new_prompt.setdefault('examples',[])
            choice=ActionFactory._get_unique_choice(examples,ActionFactory.ACTION_LIBRARIES["examples"])
            examples.append(choice)
        return new_prompt

# ==============================================================================
# æ¨¡å— B & C: RLç¯å¢ƒç±» (å·²ä¿®æ”¹)
# ==============================================================================
class PromptOptimizerEnv:
    def __init__(self, initial_prompt: Dict[str, Any]):
        self.initial_prompt = initial_prompt
        self.embedding_dim = ENCODER_MODEL.get_sentence_embedding_dimension()
        self.action_space = list(ActionFactory.ACTION_MAP.keys())
        self.reset() # resetä¸­ä¼šè°ƒç”¨_calculate_fitnessï¼Œæ‰€ä»¥æ”¾åœ¨æœ€å

    def reset(self):
        self.current_prompt = copy.deepcopy(self.initial_prompt)
        print("æ­£åœ¨è¿›è¡Œåˆå§‹é€‚åº”åº¦è¯„ä¼°...")
        self.metadata = {
            "fitness": self._calculate_fitness(self.current_prompt),
            "iterations_stuck": 0
        }
        print(f"åˆå§‹é€‚åº”åº¦è¯„ä¼°å®Œæˆ: {self.metadata['fitness']}/10")
        return self._create_state_vector()

    def _calculate_fitness(self, prompt: Dict[str, Any]) -> float:
        print("    - [LLM] æ­£åœ¨è°ƒç”¨ç”Ÿæˆæ¨¡å‹...")
        original_task = prompt.get("task", "")
        response_text = call_generator_llm(prompt)
        if not response_text:
            print("    - [LLM] ç”Ÿæˆæ¨¡å‹æœªèƒ½è¿”å›æœ‰æ•ˆå›ç­”ï¼Œé€‚åº”åº¦ä¸º0ã€‚")
            return 0.0
        print("    - [LLM] æ­£åœ¨è°ƒç”¨è¯„ä¼°æ¨¡å‹...")
        score = call_evaluator_llm(original_task, response_text)
        print(f"    - [LLM] è¯„ä¼°æ¨¡å‹ç»™å‡ºçš„åˆ†æ•°ä¸º: {score}/10")
        return float(score)

    def _normalize_features(self, features: np.ndarray) -> np.ndarray:
        max_values = np.array([10.0, 2000, 10, 50])
        safe_max_values = np.where(max_values == 0, 1, max_values)
        return features / safe_max_values

    def _create_state_vector(self) -> np.ndarray:
        prompt = self.current_prompt
        role_vector = ENCODER_MODEL.encode(prompt.get('role', ''))
        task_vector = ENCODER_MODEL.encode(prompt.get('task', ''))
        if prompt.get('constraints'):
            constraint_vectors = ENCODER_MODEL.encode(prompt['constraints'])
            avg_constraint_vector = np.mean(constraint_vectors, axis=0)
        else:
            avg_constraint_vector = np.zeros(self.embedding_dim)
        raw_context_features = np.array([
            self.metadata.get('fitness', 0.0),
            len(json.dumps(prompt)),
            len(prompt.get('constraints', [])),
            self.metadata.get('iterations_stuck', 0)
        ])
        normalized_context_features = self._normalize_features(raw_context_features)
        empty_vectors = np.zeros((2, self.embedding_dim))
        return np.concatenate([
            role_vector, task_vector, avg_constraint_vector,
            *empty_vectors, normalized_context_features
        ])

    def step(self, action_id: int) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:
        if action_id not in self.action_space:
            raise ValueError(f"æ— æ•ˆçš„åŠ¨ä½œID: {action_id}")
        old_prompt = self.current_prompt
        old_fitness = self.metadata['fitness']
        new_prompt = ActionFactory.apply_action(action_id, old_prompt)
        new_fitness = self._calculate_fitness(new_prompt)
        reward = new_fitness - old_fitness
        if reward > 0:
            self.current_prompt = new_prompt
            self.metadata['fitness'] = new_fitness
            self.metadata['iterations_stuck'] = 0
        else:
            self.metadata['iterations_stuck'] += 1
        new_state_vector = self._create_state_vector()
        done = False
        info = {
            'action_name': ActionFactory.ACTION_MAP[action_id],
            'old_fitness': old_fitness,
            'new_fitness': new_fitness,
            'reward': round(reward, 4),
            'accepted_change': reward > 0
        }
        return new_state_vector, reward, done, info

# ==============================================================================
# æ¨¡å— D: DQN Agent çš„å®ç° (æœªæ”¹å˜ï¼Œæ­¤å¤„çœç•¥)
# ==============================================================================
BATCH_SIZE=16
GAMMA=0.99
EPS_START=0.9
EPS_END=0.05
EPS_DECAY=200
LR=0.0001
TARGET_UPDATE=10
Transition=namedtuple('Transition',('state','action','next_state','reward'))
class DQN(nn.Module):
    def __init__(self,n_observations,n_actions):
        super(DQN,self).__init__()
        self.layer1=nn.Linear(n_observations,128)
        self.layer2=nn.Linear(128,128)
        self.layer3=nn.Linear(128,n_actions)
    def forward(self,x):
        x=torch.relu(self.layer1(x))
        x=torch.relu(self.layer2(x))
        return self.layer3(x)
class ReplayMemory(object):
    def __init__(self,capacity):self.memory=deque([],maxlen=capacity)
    def push(self,*args):self.memory.append(Transition(*args))
    def sample(self,batch_size):return random.sample(self.memory,batch_size)
    def __len__(self):return len(self.memory)
class DQNAgent:
    def __init__(self,state_dim,action_dim):
        self.state_dim=state_dim
        self.action_dim=action_dim
        self.policy_net=DQN(state_dim,action_dim)
        self.target_net=DQN(state_dim,action_dim)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        self.optimizer=optim.AdamW(self.policy_net.parameters(),lr=LR,amsgrad=True)
        self.memory=ReplayMemory(10000)
        self.steps_done=0
    def select_action(self,state):
        sample=random.random()
        eps_threshold=EPS_END+(EPS_START-EPS_END)*math.exp(-1.0*self.steps_done/EPS_DECAY)
        self.steps_done+=1
        if sample>eps_threshold:
            with torch.no_grad():return self.policy_net(state).max(1)[1].view(1,1)
        else:return torch.tensor([[random.randrange(self.action_dim)]],dtype=torch.long)
    def optimize_model(self):
        if len(self.memory)<BATCH_SIZE:return
        transitions=self.memory.sample(BATCH_SIZE)
        batch=Transition(*zip(*transitions))
        non_final_mask=torch.tensor(tuple(map(lambda s:s is not None,batch.next_state)),dtype=torch.bool)
        non_final_next_states=torch.cat([s for s in batch.next_state if s is not None])
        state_batch=torch.cat(batch.state)
        action_batch=torch.cat(batch.action)
        reward_batch=torch.cat(batch.reward)
        state_action_values=self.policy_net(state_batch).gather(1,action_batch)
        next_state_values=torch.zeros(BATCH_SIZE)
        with torch.no_grad():next_state_values[non_final_mask]=self.target_net(non_final_next_states).max(1)[0]
        expected_state_action_values=(next_state_values*GAMMA)+reward_batch
        criterion=nn.SmoothL1Loss()
        loss=criterion(state_action_values,expected_state_action_values.unsqueeze(1))
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(),100)
        self.optimizer.step()
    def update_target_net(self):self.target_net.load_state_dict(self.policy_net.state_dict())

# ==============================================================================
# æœ€ç»ˆæ•´åˆï¼šä½¿ç”¨çœŸå®LLMè¿›è¡ŒRLä¼˜åŒ–å¾ªç¯
# ==============================================================================
if __name__ == "__main__":
    initial_prompt_config = {
        "role": "You are a helpful assistant.",
        "task": "Explain what a neural network is in a simple way.",
        "constraints": []
    }
    
    env = PromptOptimizerEnv(initial_prompt=initial_prompt_config)
    
    state_vec = env._create_state_vector() # è·å–åˆå§‹çŠ¶æ€å‘é‡
    state_dim = len(state_vec)
    action_dim = len(env.action_space)
    agent = DQNAgent(state_dim, action_dim)

    # å¯é€‰ï¼šåŠ è½½ä¹‹å‰è®­ç»ƒå¥½çš„æ¨¡å‹
    MODEL_PATH = "dqn_agent_real_llm.pth"
    if os.path.exists(MODEL_PATH):
        agent.policy_net.load_state_dict(torch.load(MODEL_PATH))
        agent.target_net.load_state_dict(agent.policy_net.state_dict())
        print(f"âœ… ä» {MODEL_PATH} åŠ è½½äº†å·²è®­ç»ƒçš„æ¨¡å‹ï¼")

    print("\nğŸš€ å¼ºåŒ–å­¦ä¹ å¾ªç¯å¼€å§‹ (ä½¿ç”¨çœŸå®LLMè¯„ä¼°) ğŸš€")
    print("=" * 60)
    print(f"åˆå§‹æç¤ºè¯: \n{json.dumps(env.current_prompt, indent=2)}")
    print(f"åˆå§‹é€‚åº”åº¦: {env.metadata['fitness']}/10")
    print("=" * 60)

    num_episodes = 30 # ç”±äºAPIè°ƒç”¨è€—æ—¶ä¸”å¯èƒ½äº§ç”Ÿè´¹ç”¨ï¼Œå»ºè®®å‡å°‘è¿­ä»£æ¬¡æ•°
    for i_episode in range(num_episodes):
        print(f"\n----------- Episode {i_episode+1}/{num_episodes} -----------")
        state = torch.tensor(env._create_state_vector(), dtype=torch.float32).unsqueeze(0)

        action_tensor = agent.select_action(state)
        chosen_action = action_tensor.item()
        action_name = ActionFactory.ACTION_MAP[chosen_action]
        print(f"1. [åŠ¨ä½œ A] æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ: ({chosen_action}) -> {action_name}")
        
        print("2. [æ‰§è¡Œ S->S'] ç¯å¢ƒæ­£åœ¨å¤„ç†åŠ¨ä½œ...")
        new_state_vec, reward_val, done, info = env.step(chosen_action)
        
        reward = torch.tensor([reward_val], dtype=torch.float32)
        next_state = torch.tensor(new_state_vec, dtype=torch.float32).unsqueeze(0)
        
        agent.memory.push(state, action_tensor, next_state, reward)
        agent.optimize_model()

        if i_episode % TARGET_UPDATE == 0:
            agent.update_target_net()
        
        print(f"4. [ç»“æœ] å¥–åŠ± R: {info['reward']:.4f}, {'ğŸ‘ é‡‡çº³æ–°æç¤ºè¯' if info['accepted_change'] else 'ğŸ‘ ä¿ç•™åŸæç¤ºè¯'}")
        
    print("\n="*60)
    print("ğŸ è®­ç»ƒç»“æŸ ğŸ")
    print("="*60)
    
    torch.save(agent.policy_net.state_dict(), MODEL_PATH)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {MODEL_PATH}")
    
    print(f"\næœ€ç»ˆä¼˜åŒ–åçš„æç¤ºè¯: \n{json.dumps(env.current_prompt, indent=2, ensure_ascii=False)}")
    print(f"æœ€ç»ˆé€‚åº”åº¦: {env.metadata['fitness']}/10")
