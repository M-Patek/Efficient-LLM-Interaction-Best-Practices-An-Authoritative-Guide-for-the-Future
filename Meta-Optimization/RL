# -*- coding: utf-8 -*-

# ==============================================================================
# 步骤 0: 安装和导入所有必要的库
# 如果你还没有安装这些库，请先运行这行命令:
# pip install sentence-transformers numpy torch
# ==============================================================================
import numpy as np
from sentence_transformers import SentenceTransformer
import random
import copy
import hashlib
import json # 用于美化打印输出

print("正在加载 SentenceTransformer 模型...")
# 'all-MiniLM-L6-v2' 是一个轻量且高效的模型，输出384维向量
# 这个过程可能需要几秒钟，但整个程序只需要加载一次
ENCODER_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
print("模型加载完毕。")
print("=" * 60)


# ==============================================================================
# 模块 A: 动作空间 (Action Space)
# 定义了RL智能体可以执行的所有修改提示词的动作。
# ==============================================================================

# --- 动作素材库 ---
ROLE_LIBRARY = [
    "You are a world-renowned historian specializing in the Renaissance.",
    "You are a cutting-edge astrophysicist from MIT.",
    "You are a Michelin 3-star chef with expertise in molecular gastronomy.",
    "You are a cynical but brilliant detective from a noir film."
]
CONSTRAINT_LIBRARY = [
    "Explain your reasoning step-by-step.",
    "The final answer must be a single paragraph.",
    "Avoid technical jargon and use simple language.",
    "Output the result in a JSON format with keys 'item' and 'description'.",
    "Do not mention your own identity as an AI model."
]
EXAMPLE_LIBRARY = [
    {"input": "Company: Apple Inc.", "output": "Industry: Technology"},
    {"input": "Company: Toyota", "output": "Industry: Automotive"}
]
TONE_LIBRARY = [
    "in a formal and professional tone",
    "in a friendly and conversational tone",
    "in a witty and slightly sarcastic tone",
    "in a way that a complete beginner can understand"
]

# --- 具体的动作函数 ---
def paraphrase_task(prompt: dict) -> dict:
    """动作0: 核心任务释义 (模拟)"""
    new_prompt = copy.deepcopy(prompt)
    # 实际应用中，这里会调用LLM API来对任务进行释义
    new_prompt['task'] = f"[Paraphrased] {new_prompt['task']}"
    return new_prompt

def change_tone(prompt: dict) -> dict:
    """动作1: 改变语气/风格"""
    new_prompt = copy.deepcopy(prompt)
    chosen_tone = random.choice(TONE_LIBRARY)
    new_prompt['task'] = f"{new_prompt['task']}, {chosen_tone}."
    return new_prompt

def add_expert_role(prompt: dict) -> dict:
    """动作2: 增加/细化专家角色"""
    new_prompt = copy.deepcopy(prompt)
    new_prompt['role'] = random.choice(ROLE_LIBRARY)
    return new_prompt

def add_constraint(prompt: dict) -> dict:
    """动作3: 添加约束条件"""
    new_prompt = copy.deepcopy(prompt)
    new_prompt.setdefault('constraints', []).append(random.choice(CONSTRAINT_LIBRARY))
    return new_prompt

def add_few_shot_example(prompt: dict) -> dict:
    """动作4: 添加示例"""
    new_prompt = copy.deepcopy(prompt)
    new_prompt.setdefault('examples', []).append(random.choice(EXAMPLE_LIBRARY))
    return new_prompt

# --- 动作映射表 ---
# 将整数索引映射到动作函数，RL智能体通过输出整数来选择动作。
ACTION_MAP = {
    0: paraphrase_task,
    1: change_tone,
    2: add_expert_role,
    3: add_constraint,
    4: add_few_shot_example,
}


# ==============================================================================
# 模块 B: 状态表征 (State Representation)
# 定义了如何将文本提示词和元数据转换为RL智能体能理解的数字向量。
# ==============================================================================

def parse_prompt(raw_prompt: str) -> dict:
    """
    简化的解析函数，将原始文本转换为结构化字典。
    在实际应用中，这一步可能需要更复杂的解析逻辑。
    为了演示，我们假设它可以从一个固定的结构中解析。
    """
    # 这是一个示例，实际循环中我们将直接使用结构化字典。
    return {
        "role": "你是一名资深营销文案。",
        "task": "为一款名为‘星尘咖啡’的新品写三条宣传标语。",
        "constraints": ["每条不超过15个字", "风格要文艺且有想象力"],
        "examples": [],
        "format": ""
    }

def reconstruct_prompt_to_raw(structured_prompt: dict) -> str:
    """将结构化字典重新组合成一个原始字符串，用于向量化。"""
    parts = []
    if structured_prompt.get('role'):
        parts.append(structured_prompt['role'])
    if structured_prompt.get('task'):
        parts.append(structured_prompt['task'])
    if structured_prompt.get('constraints'):
        parts.append("Constraints: " + ", ".join(structured_prompt['constraints']))
    # 可以根据需要添加对examples和format的处理
    return " ".join(parts)


def create_state_vector(structured_prompt: dict, metadata: dict) -> np.ndarray:
    """
    将结构化提示词和元数据转换为一个完整的状态向量。
    """
    embedding_dim = ENCODER_MODEL.get_sentence_embedding_dimension()

    # 1. 独立编码文本部分
    role_vector = ENCODER_MODEL.encode(structured_prompt.get('role', ''))
    task_vector = ENCODER_MODEL.encode(structured_prompt.get('task', ''))

    if structured_prompt.get('constraints'):
        constraint_vectors = ENCODER_MODEL.encode(structured_prompt['constraints'])
        avg_constraint_vector = np.mean(constraint_vectors, axis=0)
    else:
        avg_constraint_vector = np.zeros(embedding_dim)

    # 为了简化，我们假设examples和format为空
    avg_example_vector = np.zeros(embedding_dim)
    format_vector = np.zeros(embedding_dim)

    # 2. 提取并规范化上下文特征
    # 注意：在实际训练中，这些数值特征应该被规范化（例如缩放到0-1范围）
    context_feature_vector = np.array([
        metadata.get('fitness', 0.0),
        len(reconstruct_prompt_to_raw(structured_prompt)), # 使用字符数作为token_count的代理
        len(structured_prompt.get('constraints', [])),
        metadata.get('iterations_stuck', 0)
    ])

    # 3. 拼接所有部分，形成最终状态向量
    return np.concatenate([
        role_vector, task_vector, avg_constraint_vector,
        avg_example_vector, format_vector, context_feature_vector
    ])


# ==============================================================================
# 模块 C: 奖励计算 (Reward Calculation)
# 定义了如何评估一个动作的好坏。
# ==============================================================================

def calculate_fitness(prompt: dict) -> float:
    """
    一个模拟的适应度计算函数。它根据提示词的内容给出一个“分数”。
    - 角色越具体，分数越高。
    - 约束越多，分数越高。
    - 加入基于内容的哈希值，让相同的prompt得分稳定，模拟评估中的确定性噪声。
    """
    score = 0.5  # 基础分
    if "assistant" not in prompt.get('role', ''):
        score += 0.25 # 具体角色加分
    score += min(0.2, len(prompt.get('constraints', [])) * 0.1) # 约束加分

    # 加入基于内容的哈希值，让分数对于相同输入是确定的
    prompt_string = json.dumps(prompt, sort_keys=True)
    hash_value = int(hashlib.md5(prompt_string.encode()).hexdigest(), 16)
    randomness = (hash_value % 100) / 1000.0 # 产生一个-0.05到+0.05的稳定噪声
    
    return round(score + randomness, 4)

def calculate_reward(prompt_old: dict, prompt_new: dict) -> tuple[float, float, float]:
    """
    奖励 = 新适应度 - 旧适应度。
    正奖励表示提升，负奖励表示下降。
    """
    fitness_old = calculate_fitness(prompt_old)
    fitness_new = calculate_fitness(prompt_new)
    reward = fitness_new - fitness_old
    return fitness_new, fitness_old, round(reward, 4)

# ==============================================================================
# 最终整合：模拟完整的RL优化循环
# ==============================================================================
if __name__ == "__main__":
    # --- 初始化环境 ---
    # 设定一个初始的、有待优化的提示词
    current_prompt = {
        "role": "You are a helpful assistant.",
        "task": "Explain what a neural network is.",
        "constraints": []
    }
    # 初始化元数据
    current_metadata = {
        "fitness": calculate_fitness(current_prompt),
        "iterations_stuck": 0
    }

    print("🚀 强化学习循环模拟开始 🚀")
    print("=" * 60)
    print(f"初始提示词: \n{json.dumps(current_prompt, indent=2)}")
    print(f"初始适应度: {current_metadata['fitness']}")
    print("=" * 60)

    # 模拟运行10次迭代
    for i in range(10):
        print(f"\n----------- Iteration {i+1} -----------")

        # 1.【状态】观察当前环境，生成状态向量
        print("1. [状态 S] 正在生成当前状态的向量表示...")
        state_vector = create_state_vector(current_prompt, current_metadata)
        print(f"   - 生成状态向量，维度: {state_vector.shape}")
        # (在真实的RL中，这个向量会输入到策略网络中)

        # 2.【动作】RL智能体根据状态选择一个动作
        # (此处为模拟，因此我们随机选择一个动作)
        chosen_action_index = random.choice(list(ACTION_MAP.keys()))
        action_function = ACTION_MAP[chosen_action_index]
        print(f"2. [动作 A] 智能体选择动作: ({chosen_action_index}) -> {action_function.__name__}")

        # 3.【执行】执行动作，生成新提示词
        new_prompt = action_function(current_prompt)
        print(f"3. [执行] 生成的新提示词: \n{json.dumps(new_prompt, indent=2)}")

        # 4.【奖励】计算执行该动作后获得的奖励
        f_new, f_old, calculated_reward = calculate_reward(current_prompt, new_prompt)
        print("4. [奖励 R] 正在计算奖励...")
        print(f"   - 旧提示词适应度: {f_old}")
        print(f"   - 新提示词适应度: {f_new}")
        
        # 5.【状态转移】根据奖励更新状态
        print("5. [状态转移 S'] 正在更新状态...")
        if calculated_reward > 0:
            print(f"   - 奖励为正 ({calculated_reward:.4f}) 👍，采纳新提示词！")
            current_prompt = new_prompt
            current_metadata['fitness'] = f_new
            current_metadata['iterations_stuck'] = 0
        else:
            print(f"   - 奖励为负或零 ({calculated_reward:.4f}) 👎，保留原提示词。")
            # 适应度保持不变
            current_metadata['iterations_stuck'] += 1
        
        print(f"   - 当前卡住轮数: {current_metadata['iterations_stuck']}")
        # (在真实的RL中，(S, A, R, S') 这个完整的经历元组会被存入经验回放池用于训练)

    print("\n="*60)
    print("🏁 模拟结束 🏁")
    print("="*60)
    print(f"最终优化后的提示词: \n{json.dumps(current_prompt, indent=2)}")
    print(f"最终适应度: {current_metadata['fitness']}")
