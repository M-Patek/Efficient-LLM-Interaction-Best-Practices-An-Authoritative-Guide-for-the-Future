# ==============================================================================
# æ­¥éª¤ 0: å®‰è£…å’Œå¯¼å…¥æ‰€æœ‰å¿…è¦çš„åº“
# å¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£…è¿™äº›åº“ï¼Œè¯·å…ˆè¿è¡Œè¿™è¡Œå‘½ä»¤:
# pip install sentence-transformers numpy torch
# ==============================================================================
import numpy as np
from sentence_transformers import SentenceTransformer
import random
import copy
import hashlib
import json
from typing import Dict, Any, List, Tuple

print("æ­£åœ¨åŠ è½½ SentenceTransformer æ¨¡å‹...")
ENCODER_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
print("æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚")
print("=" * 60)


# ==============================================================================
# æ¨¡å— A (é‡æ„): é…ç½®ä¸åŠ¨ä½œç©ºé—´
# å°†æ‰€æœ‰å¯é…ç½®çš„ç´ æåº“é›†ä¸­ç®¡ç†ï¼Œä¾¿äºä¿®æ”¹å’Œç»´æŠ¤ã€‚
# ==============================================================================
class ActionFactory:
    """é›†ä¸­ç®¡ç†æ‰€æœ‰åŠ¨ä½œç´ æå’ŒåŠ¨ä½œçš„å®ç°"""
    
    ACTION_LIBRARIES = {
        "roles": [
            "You are a world-renowned historian specializing in the Renaissance.",
            "You are a cutting-edge astrophysicist from MIT.",
            "You are a Michelin 3-star chef with expertise in molecular gastronomy.",
            "You are a cynical but brilliant detective from a noir film."
        ],
        "constraints": [
            "Explain your reasoning step-by-step.",
            "The final answer must be a single paragraph.",
            "Avoid technical jargon and use simple language.",
            "Output the result in a JSON format with keys 'item' and 'description'.",
            "Do not mention your own identity as an AI model."
        ],
        "examples": [
            {"input": "Company: Apple Inc.", "output": "Industry: Technology"},
            {"input": "Company: Toyota", "output": "Industry: Automotive"}
        ],
        "tones": [
            "in a formal and professional tone",
            "in a friendly and conversational tone",
            "in a witty and slightly sarcastic tone",
            "in a way that a complete beginner can understand"
        ]
    }

    # ä½¿ç”¨æšä¸¾æˆ–å¸¸é‡æ¥ä»£æ›¿â€œé­”æœ¯æ•°å­—â€0, 1, 2...
    PARAPHRASE_TASK = 0
    CHANGE_TONE = 1
    ADD_EXPERT_ROLE = 2
    ADD_CONSTRAINT = 3
    ADD_FEW_SHOT_EXAMPLE = 4
    
    ACTION_MAP = {
        PARAPHRASE_TASK: "paraphrase_task",
        CHANGE_TONE: "change_tone",
        ADD_EXPERT_ROLE: "add_expert_role",
        ADD_CONSTRAINT: "add_constraint",
        ADD_FEW_SHOT_EXAMPLE: "add_few_shot_example",
    }
    
    @staticmethod
    def _get_unique_choice(current_items: List[Any], library: List[Any]) -> Any:
        """ä»åº“ä¸­é€‰æ‹©ä¸€ä¸ªå½“å‰åˆ—è¡¨ä¸­ä¸å­˜åœ¨çš„é¡¹ï¼Œå¢å¼ºé²æ£’æ€§"""
        potential_choices = [item for item in library if item not in current_items]
        if not potential_choices:
            return random.choice(library) # å¦‚æœæ‰€æœ‰é¡¹éƒ½å·²å­˜åœ¨ï¼Œåˆ™éšæœºè¿”å›ä¸€ä¸ª
        return random.choice(potential_choices)

    @staticmethod
    def apply_action(action_id: int, prompt: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ®åŠ¨ä½œIDï¼Œå¯¹ç»™å®šçš„promptæ‰§è¡Œä¸€ä¸ªåŠ¨ä½œã€‚
        é‡æ„äº†æ‰€æœ‰åŠ¨ä½œå‡½æ•°ï¼Œä½¿å…¶æ›´åŠ é€šç”¨å’Œç®€æ´ã€‚
        """
        new_prompt = copy.deepcopy(prompt)
        
        if action_id == ActionFactory.PARAPHRASE_TASK:
            # å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨LLM APIæ¥å¯¹ä»»åŠ¡è¿›è¡Œé‡Šä¹‰
            new_prompt['task'] = f"[Paraphrased] {new_prompt.get('task', '')}"
        
        elif action_id == ActionFactory.CHANGE_TONE:
            chosen_tone = random.choice(ActionFactory.ACTION_LIBRARIES["tones"])
            new_prompt['task'] = f"{new_prompt.get('task', '')}, {chosen_tone}."
            
        elif action_id == ActionFactory.ADD_EXPERT_ROLE:
            new_prompt['role'] = random.choice(ActionFactory.ACTION_LIBRARIES["roles"])
            
        elif action_id == ActionFactory.ADD_CONSTRAINT:
            constraints = new_prompt.setdefault('constraints', [])
            choice = ActionFactory._get_unique_choice(constraints, ActionFactory.ACTION_LIBRARIES["constraints"])
            constraints.append(choice)

        elif action_id == ActionFactory.ADD_FEW_SHOT_EXAMPLE:
            examples = new_prompt.setdefault('examples', [])
            choice = ActionFactory._get_unique_choice(examples, ActionFactory.ACTION_LIBRARIES["examples"])
            examples.append(choice)
            
        return new_prompt


# ==============================================================================
# æ¨¡å— B & C & æ•´åˆ (é‡æ„): RLç¯å¢ƒç±»
# å°†çŠ¶æ€è¡¨ç¤ºã€å¥–åŠ±è®¡ç®—å’Œç¯å¢ƒé€»è¾‘å°è£…åˆ°ä¸€ä¸ªç±»ä¸­ï¼Œæ¨¡æ‹Ÿæ ‡å‡†çš„RLç¯å¢ƒæ¥å£ã€‚
# ==============================================================================
class PromptOptimizerEnv:
    """
    ä¸€ä¸ªæ¨¡æ‹Ÿçš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œç”¨äºä¼˜åŒ–æç¤ºè¯ã€‚
    è¿™ä¸ªç±»å°è£…äº†çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œè½¬ç§»é€»è¾‘ã€‚
    """
    def __init__(self, initial_prompt: Dict[str, Any]):
        self.initial_prompt = initial_prompt
        self.reset()
        self.embedding_dim = ENCODER_MODEL.get_sentence_embedding_dimension()
        self.action_space = list(ActionFactory.ACTION_MAP.keys())

    def reset(self):
        """é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€"""
        self.current_prompt = copy.deepcopy(self.initial_prompt)
        self.metadata = {
            "fitness": self._calculate_fitness(self.current_prompt),
            "iterations_stuck": 0
        }
        return self._create_state_vector()

    def _calculate_fitness(self, prompt: Dict[str, Any]) -> float:
        """æ¨¡æ‹Ÿçš„é€‚åº”åº¦è®¡ç®—å‡½æ•° (ä¸åŸç‰ˆé€»è¾‘ç›¸åŒ)"""
        score = 0.5
        if "assistant" not in prompt.get('role', ''):
            score += 0.25
        score += min(0.2, len(prompt.get('constraints', [])) * 0.1)
        
        prompt_string = json.dumps(prompt, sort_keys=True)
        hash_value = int(hashlib.md5(prompt_string.encode()).hexdigest(), 16)
        randomness = (hash_value % 100) / 1000.0
        return round(score + randomness, 4)

    def _normalize_features(self, features: np.ndarray) -> np.ndarray:
        """
        [ä¼˜åŒ–] å¯¹æ•°å€¼ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œæå‡é²æ£’æ€§ã€‚
        è¿™é‡Œä½¿ç”¨ç®€å•çš„æœ€å¤§å€¼ç¼©æ”¾ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç»Ÿè®¡æ–¹æ³•ã€‚
        """
        # å‡è®¾çš„æœ€å¤§å€¼ï¼Œå¯ä»¥æ ¹æ®æ•°æ®åˆ†å¸ƒè¿›è¡Œè°ƒæ•´
        max_values = np.array([1.0, 1000, 10, 50]) # fitness, token_count, constraints_len, stuck_iterations
        # é˜²æ­¢é™¤ä»¥é›¶
        safe_max_values = np.where(max_values == 0, 1, max_values)
        return features / safe_max_values

    def _create_state_vector(self) -> np.ndarray:
        """å°†ç»“æ„åŒ–æç¤ºè¯å’Œå…ƒæ•°æ®è½¬æ¢ä¸ºä¸€ä¸ªå®Œæ•´çš„çŠ¶æ€å‘é‡ (ä¸åŸç‰ˆé€»è¾‘ç›¸ä¼¼)"""
        prompt = self.current_prompt
        
        # 1. ç¼–ç æ–‡æœ¬éƒ¨åˆ†
        role_vector = ENCODER_MODEL.encode(prompt.get('role', ''))
        task_vector = ENCODER_MODEL.encode(prompt.get('task', ''))
        
        if prompt.get('constraints'):
            constraint_vectors = ENCODER_MODEL.encode(prompt['constraints'])
            avg_constraint_vector = np.mean(constraint_vectors, axis=0)
        else:
            avg_constraint_vector = np.zeros(self.embedding_dim)
        
        # 2. æå–å¹¶å½’ä¸€åŒ–ä¸Šä¸‹æ–‡ç‰¹å¾
        raw_context_features = np.array([
            self.metadata.get('fitness', 0.0),
            len(json.dumps(prompt)), # ä½¿ç”¨JSONå­—ç¬¦ä¸²é•¿åº¦ä½œä¸ºtoken_countçš„ä»£ç†
            len(prompt.get('constraints', [])),
            self.metadata.get('iterations_stuck', 0)
        ])
        
        normalized_context_features = self._normalize_features(raw_context_features)
        
        # 3. æ‹¼æ¥æ‰€æœ‰éƒ¨åˆ†
        # [ä¼˜åŒ–] å°†0å‘é‡åˆ›å»ºå’Œæ‹¼æ¥é€»è¾‘ç®€åŒ–
        empty_vectors = np.zeros((2, self.embedding_dim)) # for examples and format
        return np.concatenate([
            role_vector, task_vector, avg_constraint_vector,
            *empty_vectors, normalized_context_features
        ])

    def step(self, action_id: int) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:
        """
        æ‰§è¡Œä¸€æ­¥æ“ä½œï¼Œè¿™æ˜¯RLç¯å¢ƒçš„æ ¸å¿ƒã€‚
        è¿”å›: new_state, reward, done, info
        """
        if action_id not in self.action_space:
            raise ValueError(f"æ— æ•ˆçš„åŠ¨ä½œID: {action_id}")
            
        old_prompt = self.current_prompt
        old_fitness = self.metadata['fitness']
        
        # æ‰§è¡ŒåŠ¨ä½œ
        new_prompt = ActionFactory.apply_action(action_id, old_prompt)
        
        # è®¡ç®—å¥–åŠ±
        new_fitness = self._calculate_fitness(new_prompt)
        reward = new_fitness - old_fitness
        
        # çŠ¶æ€è½¬ç§»
        if reward > 0:
            self.current_prompt = new_prompt
            self.metadata['fitness'] = new_fitness
            self.metadata['iterations_stuck'] = 0
        else:
            self.metadata['iterations_stuck'] += 1
            
        # ç”Ÿæˆæ–°çŠ¶æ€çš„å‘é‡
        new_state_vector = self._create_state_vector()
        
        # done æ ‡å¿—åœ¨è¿™é‡Œåªæ˜¯æ¨¡æ‹Ÿï¼Œå¯ä»¥è®¾ä¸ºFalse
        done = False 
        
        # info å­—å…¸å¯ä»¥ç”¨æ¥ä¼ é€’è°ƒè¯•ä¿¡æ¯
        info = {
            'action_name': ActionFactory.ACTION_MAP[action_id],
            'old_fitness': old_fitness,
            'new_fitness': new_fitness,
            'reward': round(reward, 4),
            'accepted_change': reward > 0
        }
        
        return new_state_vector, reward, done, info

# ==============================================================================
# æœ€ç»ˆæ•´åˆï¼šæ¨¡æ‹Ÿå®Œæ•´çš„RLä¼˜åŒ–å¾ªç¯ (ä½¿ç”¨æ–°ç¯å¢ƒç±»)
# ==============================================================================
if __name__ == "__main__":
    # --- åˆå§‹åŒ–ç¯å¢ƒ ---
    initial_prompt_config = {
        "role": "You are a helpful assistant.",
        "task": "Explain what a neural network is.",
        "constraints": []
    }
    
    env = PromptOptimizerEnv(initial_prompt=initial_prompt_config)
    state = env.reset()

    print("ğŸš€ å¼ºåŒ–å­¦ä¹ å¾ªç¯æ¨¡æ‹Ÿå¼€å§‹ (ä½¿ç”¨ç¯å¢ƒç±») ğŸš€")
    print("=" * 60)
    print(f"åˆå§‹æç¤ºè¯: \n{json.dumps(env.current_prompt, indent=2)}")
    print(f"åˆå§‹é€‚åº”åº¦: {env.metadata['fitness']}")
    print("=" * 60)

    # æ¨¡æ‹Ÿè¿è¡Œ10æ¬¡è¿­ä»£
    for i in range(10):
        print(f"\n----------- Iteration {i+1} -----------")

        # 1.ã€çŠ¶æ€ Sã€‘å½“å‰çŠ¶æ€å‘é‡å·²çŸ¥
        print(f"1. [çŠ¶æ€ S] è§‚å¯Ÿåˆ°çŠ¶æ€å‘é‡ï¼Œç»´åº¦: {state.shape}")
        
        # 2.ã€åŠ¨ä½œ Aã€‘æ™ºèƒ½ä½“æ ¹æ®çŠ¶æ€é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ (éšæœºé€‰æ‹©)
        chosen_action = random.choice(env.action_space)
        action_name = ActionFactory.ACTION_MAP[chosen_action]
        print(f"2. [åŠ¨ä½œ A] æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ: ({chosen_action}) -> {action_name}")

        # 3.ã€æ‰§è¡Œ & å¥–åŠ± & çŠ¶æ€è½¬ç§»ã€‘ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›ç»“æœ
        print("3. [æ‰§è¡Œ S->S'] ç¯å¢ƒæ­£åœ¨å¤„ç†åŠ¨ä½œ...")
        new_state, reward, done, info = env.step(chosen_action)
        
        # 4. æ‰“å°ç»“æœ
        print("4. [ç»“æœ] ä»ç¯å¢ƒä¸­è·å¾—åé¦ˆ:")
        print(f"    - æ—§é€‚åº”åº¦: {info['old_fitness']:.4f} -> æ–°é€‚åº”åº¦: {info['new_fitness']:.4f}")
        print(f"    - å¥–åŠ± R: {info['reward']:.4f}")
        if info['accepted_change']:
            print("    - ç»“æœ: ğŸ‘ é‡‡çº³æ–°æç¤ºè¯ï¼")
            print(f"    - æ›´æ–°åæç¤ºè¯: \n{json.dumps(env.current_prompt, indent=2, ensure_ascii=False)}")
        else:
            print("    - ç»“æœ: ğŸ‘ ä¿ç•™åŸæç¤ºè¯ã€‚")
        print(f"    - å½“å‰å¡ä½è½®æ•°: {env.metadata['iterations_stuck']}")
        
        # æ›´æ–°çŠ¶æ€ä»¥è¿›è¡Œä¸‹ä¸€è½®
        state = new_state

    print("\n="*60)
    print("ğŸ æ¨¡æ‹Ÿç»“æŸ ğŸ")
    print("="*60)
    print(f"æœ€ç»ˆä¼˜åŒ–åçš„æç¤ºè¯: \n{json.dumps(env.current_prompt, indent=2, ensure_ascii=False)}")
    print(f"æœ€ç»ˆé€‚åº”åº¦: {env.metadata['fitness']}")
