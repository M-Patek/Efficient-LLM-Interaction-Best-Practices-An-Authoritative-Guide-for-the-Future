# ==============================================================================
# 步骤 0: 安装和导入所有必要的库
# ==============================================================================
import numpy as np
from sentence_transformers import SentenceTransformer
import random
import copy
import hashlib
import json
from typing import Dict, Any, List, Tuple
import math
import torch
import torch.nn as nn
import torch.optim as optim
from collections import namedtuple, deque

print("正在加载 SentenceTransformer 模型...")
ENCODER_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
print("模型加载完毕。")
print("=" * 60)


# ==============================================================================
# 模块 A: 配置与动作空间
# ==============================================================================
class ActionFactory:
    """集中管理所有动作素材和动作的实现"""
    
    ACTION_LIBRARIES = {
        "roles": [
            "You are a world-renowned historian specializing in the Renaissance.",
            "You are a cutting-edge astrophysicist from MIT.",
            "You are a Michelin 3-star chef with expertise in molecular gastronomy.",
            "You are a cynical but brilliant detective from a noir film."
        ],
        "constraints": [
            "Explain your reasoning step-by-step.",
            "The final answer must be a single paragraph.",
            "Avoid technical jargon and use simple language.",
            "Output the result in a JSON format with keys 'item' and 'description'.",
            "Do not mention your own identity as an AI model."
        ],
        "examples": [
            {"input": "Company: Apple Inc.", "output": "Industry: Technology"},
            {"input": "Company: Toyota", "output": "Industry: Automotive"}
        ],
        "tones": [
            "in a formal and professional tone",
            "in a friendly and conversational tone",
            "in a witty and slightly sarcastic tone",
            "in a way that a complete beginner can understand"
        ]
    }

    PARAPHRASE_TASK = 0
    CHANGE_TONE = 1
    ADD_EXPERT_ROLE = 2
    ADD_CONSTRAINT = 3
    ADD_FEW_SHOT_EXAMPLE = 4
    
    ACTION_MAP = {
        PARAPHRASE_TASK: "paraphrase_task",
        CHANGE_TONE: "change_tone",
        ADD_EXPERT_ROLE: "add_expert_role",
        ADD_CONSTRAINT: "add_constraint",
        ADD_FEW_SHOT_EXAMPLE: "add_few_shot_example",
    }
    
    @staticmethod
    def _get_unique_choice(current_items: List[Any], library: List[Any]) -> Any:
        potential_choices = [item for item in library if item not in current_items]
        if not potential_choices:
            return random.choice(library)
        return random.choice(potential_choices)

    @staticmethod
    def apply_action(action_id: int, prompt: Dict[str, Any]) -> Dict[str, Any]:
        new_prompt = copy.deepcopy(prompt)
        
        if action_id == ActionFactory.PARAPHRASE_TASK:
            new_prompt['task'] = f"[Paraphrased] {new_prompt.get('task', '')}"
        elif action_id == ActionFactory.CHANGE_TONE:
            chosen_tone = random.choice(ActionFactory.ACTION_LIBRARIES["tones"])
            new_prompt['task'] = f"{new_prompt.get('task', '')}, {chosen_tone}."
        elif action_id == ActionFactory.ADD_EXPERT_ROLE:
            new_prompt['role'] = random.choice(ActionFactory.ACTION_LIBRARIES["roles"])
        elif action_id == ActionFactory.ADD_CONSTRAINT:
            constraints = new_prompt.setdefault('constraints', [])
            choice = ActionFactory._get_unique_choice(constraints, ActionFactory.ACTION_LIBRARIES["constraints"])
            constraints.append(choice)
        elif action_id == ActionFactory.ADD_FEW_SHOT_EXAMPLE:
            examples = new_prompt.setdefault('examples', [])
            choice = ActionFactory._get_unique_choice(examples, ActionFactory.ACTION_LIBRARIES["examples"])
            examples.append(choice)
            
        return new_prompt


# ==============================================================================
# 模块 B & C: RL环境类
# ==============================================================================
class PromptOptimizerEnv:
    """一个模拟的强化学习环境，用于优化提示词。"""
    def __init__(self, initial_prompt: Dict[str, Any]):
        self.initial_prompt = initial_prompt
        self.reset()
        self.embedding_dim = ENCODER_MODEL.get_sentence_embedding_dimension()
        self.action_space = list(ActionFactory.ACTION_MAP.keys())

    def reset(self):
        self.current_prompt = copy.deepcopy(self.initial_prompt)
        self.metadata = {
            "fitness": self._calculate_fitness(self.current_prompt),
            "iterations_stuck": 0
        }
        return self._create_state_vector()

    def _calculate_fitness(self, prompt: Dict[str, Any]) -> float:
        score = 0.5
        if "assistant" not in prompt.get('role', ''):
            score += 0.25
        score += min(0.2, len(prompt.get('constraints', [])) * 0.1)
        prompt_string = json.dumps(prompt, sort_keys=True)
        hash_value = int(hashlib.md5(prompt_string.encode()).hexdigest(), 16)
        randomness = (hash_value % 100) / 1000.0
        return round(score + randomness, 4)

    def _normalize_features(self, features: np.ndarray) -> np.ndarray:
        max_values = np.array([1.0, 1000, 10, 50])
        safe_max_values = np.where(max_values == 0, 1, max_values)
        return features / safe_max_values

    def _create_state_vector(self) -> np.ndarray:
        prompt = self.current_prompt
        role_vector = ENCODER_MODEL.encode(prompt.get('role', ''))
        task_vector = ENCODER_MODEL.encode(prompt.get('task', ''))
        
        if prompt.get('constraints'):
            constraint_vectors = ENCODER_MODEL.encode(prompt['constraints'])
            avg_constraint_vector = np.mean(constraint_vectors, axis=0)
        else:
            avg_constraint_vector = np.zeros(self.embedding_dim)
        
        raw_context_features = np.array([
            self.metadata.get('fitness', 0.0),
            len(json.dumps(prompt)),
            len(prompt.get('constraints', [])),
            self.metadata.get('iterations_stuck', 0)
        ])
        
        normalized_context_features = self._normalize_features(raw_context_features)
        
        empty_vectors = np.zeros((2, self.embedding_dim))
        return np.concatenate([
            role_vector, task_vector, avg_constraint_vector,
            *empty_vectors, normalized_context_features
        ])

    def step(self, action_id: int) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:
        if action_id not in self.action_space:
            raise ValueError(f"无效的动作ID: {action_id}")
            
        old_prompt = self.current_prompt
        old_fitness = self.metadata['fitness']
        
        new_prompt = ActionFactory.apply_action(action_id, old_prompt)
        new_fitness = self._calculate_fitness(new_prompt)
        reward = new_fitness - old_fitness
        
        if reward > 0:
            self.current_prompt = new_prompt
            self.metadata['fitness'] = new_fitness
            self.metadata['iterations_stuck'] = 0
        else:
            self.metadata['iterations_stuck'] += 1
            
        new_state_vector = self._create_state_vector()
        done = False 
        info = {
            'action_name': ActionFactory.ACTION_MAP[action_id],
            'old_fitness': old_fitness,
            'new_fitness': new_fitness,
            'reward': round(reward, 4),
            'accepted_change': reward > 0
        }
        return new_state_vector, reward, done, info

# ==============================================================================
# 模块 D: DQN Agent 的实现
# ==============================================================================

# --- 定义超参数 ---
BATCH_SIZE = 16
GAMMA = 0.99
EPS_START = 0.9
EPS_END = 0.05
EPS_DECAY = 200 # 让探索率衰减得慢一些
LR = 1e-4
TARGET_UPDATE = 10 # 每10次迭代更新一次目标网络

# --- 定义经验的数据结构 ---
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class DQN(nn.Module):
    """Q网络 (The "Brain")"""
    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 128)
        self.layer2 = nn.Linear(128, 128)
        self.layer3 = nn.Linear(128, n_actions)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        return self.layer3(x)

class ReplayMemory(object):
    """经验回放池 (The "Memory")"""
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class DQNAgent:
    """DQN智能体 (The "Controller")"""
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        self.policy_net = DQN(state_dim, action_dim)
        self.target_net = DQN(state_dim, action_dim)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True)
        self.memory = ReplayMemory(10000)
        self.steps_done = 0

    def select_action(self, state):
        sample = random.random()
        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)
        self.steps_done += 1
        
        if sample > eps_threshold:
            with torch.no_grad():
                return self.policy_net(state).max(1)[1].view(1, 1)
        else:
            return torch.tensor([[random.randrange(self.action_dim)]], dtype=torch.long)

    def optimize_model(self):
        if len(self.memory) < BATCH_SIZE:
            return
        
        transitions = self.memory.sample(BATCH_SIZE)
        batch = Transition(*zip(*transitions))

        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)
        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
        
        state_batch = torch.cat(batch.state)
        action_batch = torch.cat(batch.action)
        reward_batch = torch.cat(batch.reward)

        state_action_values = self.policy_net(state_batch).gather(1, action_batch)

        next_state_values = torch.zeros(BATCH_SIZE)
        with torch.no_grad():
            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]
        
        expected_state_action_values = (next_state_values * GAMMA) + reward_batch

        criterion = nn.SmoothL1Loss()
        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))

        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)
        self.optimizer.step()

    def update_target_net(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())


# ==============================================================================
# 最终整合：使用DQN Agent进行RL优化循环
# ==============================================================================
if __name__ == "__main__":
    # --- 初始化环境 ---
    initial_prompt_config = {
        "role": "You are a helpful assistant.",
        "task": "Explain what a neural network is.",
        "constraints": []
    }
    
    env = PromptOptimizerEnv(initial_prompt=initial_prompt_config)
    
    # --- 初始化Agent ---
    state_vec = env.reset()
    state_dim = len(state_vec)
    action_dim = len(env.action_space)
    agent = DQNAgent(state_dim, action_dim)

    print("🚀 强化学习循环开始 (使用DQN Agent) 🚀")
    print("=" * 60)
    print(f"初始提示词: \n{json.dumps(env.current_prompt, indent=2)}")
    print(f"初始适应度: {env.metadata['fitness']}")
    print("=" * 60)

    num_episodes = 200
    for i_episode in range(num_episodes):
        state = torch.tensor(env._create_state_vector(), dtype=torch.float32).unsqueeze(0)

        # 1. Agent选择动作
        action_tensor = agent.select_action(state)
        chosen_action = action_tensor.item()
        
        # 2. 环境执行动作
        new_state_vec, reward_val, done, info = env.step(chosen_action)
        
        reward = torch.tensor([reward_val], dtype=torch.float32)
        next_state = torch.tensor(new_state_vec, dtype=torch.float32).unsqueeze(0)
        
        # 3. 存入记忆
        agent.memory.push(state, action_tensor, next_state, reward)

        # 4. Agent学习
        agent.optimize_model()

        # 5. 定期更新目标网络
        if i_episode % TARGET_UPDATE == 0:
            agent.update_target_net()
        
        # --- 打印日志 ---
        if (i_episode + 1) % 20 == 0:
            print(f"\n----------- Episode {i_episode+1} -----------")
            print(f"动作: {info['action_name']}")
            print(f"奖励: {info['reward']:.4f}, {'👍 采纳新提示词' if info['accepted_change'] else '👎 保留原提示词'}")
            print(f"当前适应度: {env.metadata['fitness']:.4f}")
            current_eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * agent.steps_done / EPS_DECAY)
            print(f"当前探索率 ε: {current_eps:.4f}")

    print("\n="*60)
    print("🏁 训练结束 🏁")
    print("="*60)
    print(f"最终优化后的提示词: \n{json.dumps(env.current_prompt, indent=2, ensure_ascii=False)}")
    print(f"最终适应度: {env.metadata['fitness']}")
