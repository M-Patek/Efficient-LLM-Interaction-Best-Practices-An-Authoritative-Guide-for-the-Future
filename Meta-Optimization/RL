# ==============================================================================
# æ­¥éª¤ 0: å®‰è£…å’Œå¯¼å…¥æ‰€æœ‰å¿…è¦çš„åº“
# ==============================================================================
import numpy as np
from sentence_transformers import SentenceTransformer
import random
import copy
import hashlib
import json
from typing import Dict, Any, List, Tuple
import math
import torch
import torch.nn as nn
import torch.optim as optim
from collections import namedtuple, deque

print("æ­£åœ¨åŠ è½½ SentenceTransformer æ¨¡åž‹...")
ENCODER_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
print("æ¨¡åž‹åŠ è½½å®Œæ¯•ã€‚")
print("=" * 60)


# ==============================================================================
# æ¨¡å— A: é…ç½®ä¸ŽåŠ¨ä½œç©ºé—´
# ==============================================================================
class ActionFactory:
    """é›†ä¸­ç®¡ç†æ‰€æœ‰åŠ¨ä½œç´ æå’ŒåŠ¨ä½œçš„å®žçŽ°"""
    
    ACTION_LIBRARIES = {
        "roles": [
            "You are a world-renowned historian specializing in the Renaissance.",
            "You are a cutting-edge astrophysicist from MIT.",
            "You are a Michelin 3-star chef with expertise in molecular gastronomy.",
            "You are a cynical but brilliant detective from a noir film."
        ],
        "constraints": [
            "Explain your reasoning step-by-step.",
            "The final answer must be a single paragraph.",
            "Avoid technical jargon and use simple language.",
            "Output the result in a JSON format with keys 'item' and 'description'.",
            "Do not mention your own identity as an AI model."
        ],
        "examples": [
            {"input": "Company: Apple Inc.", "output": "Industry: Technology"},
            {"input": "Company: Toyota", "output": "Industry: Automotive"}
        ],
        "tones": [
            "in a formal and professional tone",
            "in a friendly and conversational tone",
            "in a witty and slightly sarcastic tone",
            "in a way that a complete beginner can understand"
        ]
    }

    PARAPHRASE_TASK = 0
    CHANGE_TONE = 1
    ADD_EXPERT_ROLE = 2
    ADD_CONSTRAINT = 3
    ADD_FEW_SHOT_EXAMPLE = 4
    
    ACTION_MAP = {
        PARAPHRASE_TASK: "paraphrase_task",
        CHANGE_TONE: "change_tone",
        ADD_EXPERT_ROLE: "add_expert_role",
        ADD_CONSTRAINT: "add_constraint",
        ADD_FEW_SHOT_EXAMPLE: "add_few_shot_example",
    }
    
    @staticmethod
    def _get_unique_choice(current_items: List[Any], library: List[Any]) -> Any:
        potential_choices = [item for item in library if item not in current_items]
        if not potential_choices:
            return random.choice(library)
        return random.choice(potential_choices)

    @staticmethod
    def apply_action(action_id: int, prompt: Dict[str, Any]) -> Dict[str, Any]:
        new_prompt = copy.deepcopy(prompt)
        
        if action_id == ActionFactory.PARAPHRASE_TASK:
            new_prompt['task'] = f"[Paraphrased] {new_prompt.get('task', '')}"
        elif action_id == ActionFactory.CHANGE_TONE:
            chosen_tone = random.choice(ActionFactory.ACTION_LIBRARIES["tones"])
            new_prompt['task'] = f"{new_prompt.get('task', '')}, {chosen_tone}."
        elif action_id == ActionFactory.ADD_EXPERT_ROLE:
            new_prompt['role'] = random.choice(ActionFactory.ACTION_LIBRARIES["roles"])
        elif action_id == ActionFactory.ADD_CONSTRAINT:
            constraints = new_prompt.setdefault('constraints', [])
            choice = ActionFactory._get_unique_choice(constraints, ActionFactory.ACTION_LIBRARIES["constraints"])
            constraints.append(choice)
        elif action_id == ActionFactory.ADD_FEW_SHOT_EXAMPLE:
            examples = new_prompt.setdefault('examples', [])
            choice = ActionFactory._get_unique_choice(examples, ActionFactory.ACTION_LIBRARIES["examples"])
            examples.append(choice)
            
        return new_prompt


# ==============================================================================
# æ¨¡å— B & C: RLçŽ¯å¢ƒç±»
# ==============================================================================
class PromptOptimizerEnv:
    """ä¸€ä¸ªæ¨¡æ‹Ÿçš„å¼ºåŒ–å­¦ä¹ çŽ¯å¢ƒï¼Œç”¨äºŽä¼˜åŒ–æç¤ºè¯ã€‚"""
    def __init__(self, initial_prompt: Dict[str, Any]):
        self.initial_prompt = initial_prompt
        self.reset()
        self.embedding_dim = ENCODER_MODEL.get_sentence_embedding_dimension()
        self.action_space = list(ActionFactory.ACTION_MAP.keys())

    def reset(self):
        self.current_prompt = copy.deepcopy(self.initial_prompt)
        self.metadata = {
            "fitness": self._calculate_fitness(self.current_prompt),
            "iterations_stuck": 0
        }
        return self._create_state_vector()

    def _calculate_fitness(self, prompt: Dict[str, Any]) -> float:
        score = 0.5
        if "assistant" not in prompt.get('role', ''):
            score += 0.25
        score += min(0.2, len(prompt.get('constraints', [])) * 0.1)
        prompt_string = json.dumps(prompt, sort_keys=True)
        hash_value = int(hashlib.md5(prompt_string.encode()).hexdigest(), 16)
        randomness = (hash_value % 100) / 1000.0
        return round(score + randomness, 4)

    def _normalize_features(self, features: np.ndarray) -> np.ndarray:
        max_values = np.array([1.0, 1000, 10, 50])
        safe_max_values = np.where(max_values == 0, 1, max_values)
        return features / safe_max_values

    def _create_state_vector(self) -> np.ndarray:
        prompt = self.current_prompt
        role_vector = ENCODER_MODEL.encode(prompt.get('role', ''))
        task_vector = ENCODER_MODEL.encode(prompt.get('task', ''))
        
        if prompt.get('constraints'):
            constraint_vectors = ENCODER_MODEL.encode(prompt['constraints'])
            avg_constraint_vector = np.mean(constraint_vectors, axis=0)
        else:
            avg_constraint_vector = np.zeros(self.embedding_dim)
        
        raw_context_features = np.array([
            self.metadata.get('fitness', 0.0),
            len(json.dumps(prompt)),
            len(prompt.get('constraints', [])),
            self.metadata.get('iterations_stuck', 0)
        ])
        
        normalized_context_features = self._normalize_features(raw_context_features)
        
        empty_vectors = np.zeros((2, self.embedding_dim))
        return np.concatenate([
            role_vector, task_vector, avg_constraint_vector,
            *empty_vectors, normalized_context_features
        ])

    def step(self, action_id: int) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:
        if action_id not in self.action_space:
            raise ValueError(f"æ— æ•ˆçš„åŠ¨ä½œID: {action_id}")
            
        old_prompt = self.current_prompt
        old_fitness = self.metadata['fitness']
        
        new_prompt = ActionFactory.apply_action(action_id, old_prompt)
        new_fitness = self._calculate_fitness(new_prompt)
        reward = new_fitness - old_fitness
        
        if reward > 0:
            self.current_prompt = new_prompt
            self.metadata['fitness'] = new_fitness
            self.metadata['iterations_stuck'] = 0
        else:
            self.metadata['iterations_stuck'] += 1
            
        new_state_vector = self._create_state_vector()
        done = False 
        info = {
            'action_name': ActionFactory.ACTION_MAP[action_id],
            'old_fitness': old_fitness,
            'new_fitness': new_fitness,
            'reward': round(reward, 4),
            'accepted_change': reward > 0
        }
        return new_state_vector, reward, done, info

# ==============================================================================
# æ¨¡å— D: DQN Agent çš„å®žçŽ°
# ==============================================================================

# --- å®šä¹‰è¶…å‚æ•° ---
BATCH_SIZE = 16
GAMMA = 0.99
EPS_START = 0.9
EPS_END = 0.05
EPS_DECAY = 200 # è®©æŽ¢ç´¢çŽ‡è¡°å‡å¾—æ…¢ä¸€äº›
LR = 1e-4
TARGET_UPDATE = 10 # æ¯10æ¬¡è¿­ä»£æ›´æ–°ä¸€æ¬¡ç›®æ ‡ç½‘ç»œ

# --- å®šä¹‰ç»éªŒçš„æ•°æ®ç»“æž„ ---
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class DQN(nn.Module):
    """Qç½‘ç»œ (The "Brain")"""
    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, 128)
        self.layer2 = nn.Linear(128, 128)
        self.layer3 = nn.Linear(128, n_actions)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        return self.layer3(x)

class ReplayMemory(object):
    """ç»éªŒå›žæ”¾æ±  (The "Memory")"""
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class DQNAgent:
    """DQNæ™ºèƒ½ä½“ (The "Controller")"""
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        self.policy_net = DQN(state_dim, action_dim)
        self.target_net = DQN(state_dim, action_dim)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True)
        self.memory = ReplayMemory(10000)
        self.steps_done = 0

    def select_action(self, state):
        sample = random.random()
        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)
        self.steps_done += 1
        
        if sample > eps_threshold:
            with torch.no_grad():
                return self.policy_net(state).max(1)[1].view(1, 1)
        else:
            return torch.tensor([[random.randrange(self.action_dim)]], dtype=torch.long)

    def optimize_model(self):
        if len(self.memory) < BATCH_SIZE:
            return
        
        transitions = self.memory.sample(BATCH_SIZE)
        batch = Transition(*zip(*transitions))

        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)
        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
        
        state_batch = torch.cat(batch.state)
        action_batch = torch.cat(batch.action)
        reward_batch = torch.cat(batch.reward)

        state_action_values = self.policy_net(state_batch).gather(1, action_batch)

        next_state_values = torch.zeros(BATCH_SIZE)
        with torch.no_grad():
            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]
        
        expected_state_action_values = (next_state_values * GAMMA) + reward_batch

        criterion = nn.SmoothL1Loss()
        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))

        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)
        self.optimizer.step()

    def update_target_net(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())


# ==============================================================================
# æœ€ç»ˆæ•´åˆï¼šä½¿ç”¨DQN Agentè¿›è¡ŒRLä¼˜åŒ–å¾ªçŽ¯
# ==============================================================================
if __name__ == "__main__":
    # --- åˆå§‹åŒ–çŽ¯å¢ƒ ---
    initial_prompt_config = {
        "role": "You are a helpful assistant.",
        "task": "Explain what a neural network is.",
        "constraints": []
    }
    
    env = PromptOptimizerEnv(initial_prompt=initial_prompt_config)
    
    # --- åˆå§‹åŒ–Agent ---
    state_vec = env.reset()
    state_dim = len(state_vec)
    action_dim = len(env.action_space)
    agent = DQNAgent(state_dim, action_dim)

    print("ðŸš€ å¼ºåŒ–å­¦ä¹ å¾ªçŽ¯å¼€å§‹ (ä½¿ç”¨DQN Agent) ðŸš€")
    print("=" * 60)
    print(f"åˆå§‹æç¤ºè¯: \n{json.dumps(env.current_prompt, indent=2)}")
    print(f"åˆå§‹é€‚åº”åº¦: {env.metadata['fitness']}")
    print("=" * 60)

    num_episodes = 200
    for i_episode in range(num_episodes):
        state = torch.tensor(env._create_state_vector(), dtype=torch.float32).unsqueeze(0)

        # 1. Agenté€‰æ‹©åŠ¨ä½œ
        action_tensor = agent.select_action(state)
        chosen_action = action_tensor.item()
        
        # 2. çŽ¯å¢ƒæ‰§è¡ŒåŠ¨ä½œ
        new_state_vec, reward_val, done, info = env.step(chosen_action)
        
        reward = torch.tensor([reward_val], dtype=torch.float32)
        next_state = torch.tensor(new_state_vec, dtype=torch.float32).unsqueeze(0)
        
        # 3. å­˜å…¥è®°å¿†
        agent.memory.push(state, action_tensor, next_state, reward)

        # 4. Agentå­¦ä¹ 
        agent.optimize_model()

        # 5. å®šæœŸæ›´æ–°ç›®æ ‡ç½‘ç»œ
        if i_episode % TARGET_UPDATE == 0:
            agent.update_target_net()
        
        # --- æ‰“å°æ—¥å¿— ---
        if (i_episode + 1) % 20 == 0:
            print(f"\n----------- Episode {i_episode+1} -----------")
            print(f"åŠ¨ä½œ: {info['action_name']}")
            print(f"å¥–åŠ±: {info['reward']:.4f}, {'ðŸ‘ é‡‡çº³æ–°æç¤ºè¯' if info['accepted_change'] else 'ðŸ‘Ž ä¿ç•™åŽŸæç¤ºè¯'}")
            print(f"å½“å‰é€‚åº”åº¦: {env.metadata['fitness']:.4f}")
            current_eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * agent.steps_done / EPS_DECAY)
            print(f"å½“å‰æŽ¢ç´¢çŽ‡ Îµ: {current_eps:.4f}")

    print("\n="*60)
    print("ðŸ è®­ç»ƒç»“æŸ ðŸ")
    print("="*60)
    print(f"æœ€ç»ˆä¼˜åŒ–åŽçš„æç¤ºè¯: \n{json.dumps(env.current_prompt, indent=2, ensure_ascii=False)}")
    print(f"æœ€ç»ˆé€‚åº”åº¦: {env.metadata['fitness']}")
