# ==============================================================================
# 步骤 0: 安装和导入所有必要的库
# 如果你还没有安装这些库，请先运行这行命令:
# pip install sentence-transformers numpy torch
# ==============================================================================
import numpy as np
from sentence_transformers import SentenceTransformer
import random
import copy
import hashlib
import json
from typing import Dict, Any, List, Tuple

print("正在加载 SentenceTransformer 模型...")
ENCODER_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
print("模型加载完毕。")
print("=" * 60)


# ==============================================================================
# 模块 A (重构): 配置与动作空间
# 将所有可配置的素材库集中管理，便于修改和维护。
# ==============================================================================
class ActionFactory:
    """集中管理所有动作素材和动作的实现"""
    
    ACTION_LIBRARIES = {
        "roles": [
            "You are a world-renowned historian specializing in the Renaissance.",
            "You are a cutting-edge astrophysicist from MIT.",
            "You are a Michelin 3-star chef with expertise in molecular gastronomy.",
            "You are a cynical but brilliant detective from a noir film."
        ],
        "constraints": [
            "Explain your reasoning step-by-step.",
            "The final answer must be a single paragraph.",
            "Avoid technical jargon and use simple language.",
            "Output the result in a JSON format with keys 'item' and 'description'.",
            "Do not mention your own identity as an AI model."
        ],
        "examples": [
            {"input": "Company: Apple Inc.", "output": "Industry: Technology"},
            {"input": "Company: Toyota", "output": "Industry: Automotive"}
        ],
        "tones": [
            "in a formal and professional tone",
            "in a friendly and conversational tone",
            "in a witty and slightly sarcastic tone",
            "in a way that a complete beginner can understand"
        ]
    }

    # 使用枚举或常量来代替“魔术数字”0, 1, 2...
    PARAPHRASE_TASK = 0
    CHANGE_TONE = 1
    ADD_EXPERT_ROLE = 2
    ADD_CONSTRAINT = 3
    ADD_FEW_SHOT_EXAMPLE = 4
    
    ACTION_MAP = {
        PARAPHRASE_TASK: "paraphrase_task",
        CHANGE_TONE: "change_tone",
        ADD_EXPERT_ROLE: "add_expert_role",
        ADD_CONSTRAINT: "add_constraint",
        ADD_FEW_SHOT_EXAMPLE: "add_few_shot_example",
    }
    
    @staticmethod
    def _get_unique_choice(current_items: List[Any], library: List[Any]) -> Any:
        """从库中选择一个当前列表中不存在的项，增强鲁棒性"""
        potential_choices = [item for item in library if item not in current_items]
        if not potential_choices:
            return random.choice(library) # 如果所有项都已存在，则随机返回一个
        return random.choice(potential_choices)

    @staticmethod
    def apply_action(action_id: int, prompt: Dict[str, Any]) -> Dict[str, Any]:
        """
        根据动作ID，对给定的prompt执行一个动作。
        重构了所有动作函数，使其更加通用和简洁。
        """
        new_prompt = copy.deepcopy(prompt)
        
        if action_id == ActionFactory.PARAPHRASE_TASK:
            # 实际应用中，这里会调用LLM API来对任务进行释义
            new_prompt['task'] = f"[Paraphrased] {new_prompt.get('task', '')}"
        
        elif action_id == ActionFactory.CHANGE_TONE:
            chosen_tone = random.choice(ActionFactory.ACTION_LIBRARIES["tones"])
            new_prompt['task'] = f"{new_prompt.get('task', '')}, {chosen_tone}."
            
        elif action_id == ActionFactory.ADD_EXPERT_ROLE:
            new_prompt['role'] = random.choice(ActionFactory.ACTION_LIBRARIES["roles"])
            
        elif action_id == ActionFactory.ADD_CONSTRAINT:
            constraints = new_prompt.setdefault('constraints', [])
            choice = ActionFactory._get_unique_choice(constraints, ActionFactory.ACTION_LIBRARIES["constraints"])
            constraints.append(choice)

        elif action_id == ActionFactory.ADD_FEW_SHOT_EXAMPLE:
            examples = new_prompt.setdefault('examples', [])
            choice = ActionFactory._get_unique_choice(examples, ActionFactory.ACTION_LIBRARIES["examples"])
            examples.append(choice)
            
        return new_prompt


# ==============================================================================
# 模块 B & C & 整合 (重构): RL环境类
# 将状态表示、奖励计算和环境逻辑封装到一个类中，模拟标准的RL环境接口。
# ==============================================================================
class PromptOptimizerEnv:
    """
    一个模拟的强化学习环境，用于优化提示词。
    这个类封装了状态、动作、奖励和转移逻辑。
    """
    def __init__(self, initial_prompt: Dict[str, Any]):
        self.initial_prompt = initial_prompt
        self.reset()
        self.embedding_dim = ENCODER_MODEL.get_sentence_embedding_dimension()
        self.action_space = list(ActionFactory.ACTION_MAP.keys())

    def reset(self):
        """重置环境到初始状态"""
        self.current_prompt = copy.deepcopy(self.initial_prompt)
        self.metadata = {
            "fitness": self._calculate_fitness(self.current_prompt),
            "iterations_stuck": 0
        }
        return self._create_state_vector()

    def _calculate_fitness(self, prompt: Dict[str, Any]) -> float:
        """模拟的适应度计算函数 (与原版逻辑相同)"""
        score = 0.5
        if "assistant" not in prompt.get('role', ''):
            score += 0.25
        score += min(0.2, len(prompt.get('constraints', [])) * 0.1)
        
        prompt_string = json.dumps(prompt, sort_keys=True)
        hash_value = int(hashlib.md5(prompt_string.encode()).hexdigest(), 16)
        randomness = (hash_value % 100) / 1000.0
        return round(score + randomness, 4)

    def _normalize_features(self, features: np.ndarray) -> np.ndarray:
        """
        [优化] 对数值特征进行归一化，提升鲁棒性。
        这里使用简单的最大值缩放，实际应用中可能需要更复杂的统计方法。
        """
        # 假设的最大值，可以根据数据分布进行调整
        max_values = np.array([1.0, 1000, 10, 50]) # fitness, token_count, constraints_len, stuck_iterations
        # 防止除以零
        safe_max_values = np.where(max_values == 0, 1, max_values)
        return features / safe_max_values

    def _create_state_vector(self) -> np.ndarray:
        """将结构化提示词和元数据转换为一个完整的状态向量 (与原版逻辑相似)"""
        prompt = self.current_prompt
        
        # 1. 编码文本部分
        role_vector = ENCODER_MODEL.encode(prompt.get('role', ''))
        task_vector = ENCODER_MODEL.encode(prompt.get('task', ''))
        
        if prompt.get('constraints'):
            constraint_vectors = ENCODER_MODEL.encode(prompt['constraints'])
            avg_constraint_vector = np.mean(constraint_vectors, axis=0)
        else:
            avg_constraint_vector = np.zeros(self.embedding_dim)
        
        # 2. 提取并归一化上下文特征
        raw_context_features = np.array([
            self.metadata.get('fitness', 0.0),
            len(json.dumps(prompt)), # 使用JSON字符串长度作为token_count的代理
            len(prompt.get('constraints', [])),
            self.metadata.get('iterations_stuck', 0)
        ])
        
        normalized_context_features = self._normalize_features(raw_context_features)
        
        # 3. 拼接所有部分
        # [优化] 将0向量创建和拼接逻辑简化
        empty_vectors = np.zeros((2, self.embedding_dim)) # for examples and format
        return np.concatenate([
            role_vector, task_vector, avg_constraint_vector,
            *empty_vectors, normalized_context_features
        ])

    def step(self, action_id: int) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:
        """
        执行一步操作，这是RL环境的核心。
        返回: new_state, reward, done, info
        """
        if action_id not in self.action_space:
            raise ValueError(f"无效的动作ID: {action_id}")
            
        old_prompt = self.current_prompt
        old_fitness = self.metadata['fitness']
        
        # 执行动作
        new_prompt = ActionFactory.apply_action(action_id, old_prompt)
        
        # 计算奖励
        new_fitness = self._calculate_fitness(new_prompt)
        reward = new_fitness - old_fitness
        
        # 状态转移
        if reward > 0:
            self.current_prompt = new_prompt
            self.metadata['fitness'] = new_fitness
            self.metadata['iterations_stuck'] = 0
        else:
            self.metadata['iterations_stuck'] += 1
            
        # 生成新状态的向量
        new_state_vector = self._create_state_vector()
        
        # done 标志在这里只是模拟，可以设为False
        done = False 
        
        # info 字典可以用来传递调试信息
        info = {
            'action_name': ActionFactory.ACTION_MAP[action_id],
            'old_fitness': old_fitness,
            'new_fitness': new_fitness,
            'reward': round(reward, 4),
            'accepted_change': reward > 0
        }
        
        return new_state_vector, reward, done, info

# ==============================================================================
# 最终整合：模拟完整的RL优化循环 (使用新环境类)
# ==============================================================================
if __name__ == "__main__":
    # --- 初始化环境 ---
    initial_prompt_config = {
        "role": "You are a helpful assistant.",
        "task": "Explain what a neural network is.",
        "constraints": []
    }
    
    env = PromptOptimizerEnv(initial_prompt=initial_prompt_config)
    state = env.reset()

    print("🚀 强化学习循环模拟开始 (使用环境类) 🚀")
    print("=" * 60)
    print(f"初始提示词: \n{json.dumps(env.current_prompt, indent=2)}")
    print(f"初始适应度: {env.metadata['fitness']}")
    print("=" * 60)

    # 模拟运行10次迭代
    for i in range(10):
        print(f"\n----------- Iteration {i+1} -----------")

        # 1.【状态 S】当前状态向量已知
        print(f"1. [状态 S] 观察到状态向量，维度: {state.shape}")
        
        # 2.【动作 A】智能体根据状态选择一个动作 (随机选择)
        chosen_action = random.choice(env.action_space)
        action_name = ActionFactory.ACTION_MAP[chosen_action]
        print(f"2. [动作 A] 智能体选择动作: ({chosen_action}) -> {action_name}")

        # 3.【执行 & 奖励 & 状态转移】环境执行动作并返回结果
        print("3. [执行 S->S'] 环境正在处理动作...")
        new_state, reward, done, info = env.step(chosen_action)
        
        # 4. 打印结果
        print("4. [结果] 从环境中获得反馈:")
        print(f"    - 旧适应度: {info['old_fitness']:.4f} -> 新适应度: {info['new_fitness']:.4f}")
        print(f"    - 奖励 R: {info['reward']:.4f}")
        if info['accepted_change']:
            print("    - 结果: 👍 采纳新提示词！")
            print(f"    - 更新后提示词: \n{json.dumps(env.current_prompt, indent=2, ensure_ascii=False)}")
        else:
            print("    - 结果: 👎 保留原提示词。")
        print(f"    - 当前卡住轮数: {env.metadata['iterations_stuck']}")
        
        # 更新状态以进行下一轮
        state = new_state

    print("\n="*60)
    print("🏁 模拟结束 🏁")
    print("="*60)
    print(f"最终优化后的提示词: \n{json.dumps(env.current_prompt, indent=2, ensure_ascii=False)}")
    print(f"最终适应度: {env.metadata['fitness']}")
